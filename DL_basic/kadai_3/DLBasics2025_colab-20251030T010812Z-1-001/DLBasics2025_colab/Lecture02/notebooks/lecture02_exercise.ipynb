{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuR-Q0Bw4Kqu"
      },
      "source": [
        "# 第2回講義 演習\n",
        "\n",
        "今回は，深層モデルやそのライブラリは用いず，機械学習の基礎的な手法であるロジスティック回帰（およびソフトマックス回帰）を実装します．\n",
        "\n",
        "名前に「回帰」とありますが，分類問題であることに注意してください．シグモイド関数を用いることで，データのラベルが1または0である確率を出力します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJvQUbTP4Kq5"
      },
      "source": [
        "## 目次\n",
        "\n",
        "1. [【課題 1】ロジスティック回帰の実装と学習 (OR)](#scrollTo=quBPRwBf4Kq7&line=1&uniqifier=1)\n",
        "    \n",
        "    1.1. [シグモイド関数](#scrollTo=awSlFgRA4Kq9)\n",
        "\n",
        "    1.2. [データセットの設定と重みの定義](#scrollTo=HAA-lvhz4KrF)\n",
        "    \n",
        "    1.3. [train関数とvalid関数](#scrollTo=7raMb3ts4KrL)\n",
        "    \n",
        "    1.4. [学習](#scrollTo=LiuO_6B-4Krb)\n",
        "\n",
        "1. [【課題 2】ソフトマックス回帰の実装と学習 (MNIST)](#scrollTo=44tdPsW34Krq&line=1&uniqifier=1)\n",
        "    \n",
        "    2.1. [ソフトマックス関数](#scrollTo=YEprLDMd4Krr)\n",
        "    \n",
        "    2.2. [データセットの設定と重みの定義](#scrollTo=52fR-55x4Krx)\n",
        "    \n",
        "    2.3. [train関数とvalid関数](#scrollTo=80EOFI-n4Kr6)\n",
        "\n",
        "    2.4. [学習](#scrollTo=JBGInXhG4KsJ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AKBw3Tr4Kqx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBPRwBf4Kq7"
      },
      "source": [
        "## 1.【課題 1】ロジスティック回帰の実装と学習 (OR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awSlFgRA4Kq9"
      },
      "source": [
        "### 1.1. シグモイド関数\n",
        "$$\n",
        "    \\sigma({\\bf x}) = \\frac{1}{1 + \\exp(-{\\bf x})} = \\frac{\\exp({\\bf x})}{1 + \\exp({\\bf x})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxGov-BfwsiS"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    # 単純な実装\n",
        "    # return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # expのoverflow対策を施した実装\n",
        "    # x >=0 のとき sigmoid(x) = 1 / (1 + exp(-x))\n",
        "    # x < 0 のとき sigmoid(x) = exp(x) / (1 + exp(x))\n",
        "    return  # WRITE ME\n",
        "\n",
        "x = np.arange(-10, 10, 0.1)\n",
        "plt.plot(x, sigmoid(x.copy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAA-lvhz4KrF"
      },
      "source": [
        "### 1.2. データセットの設定と重みの定義\n",
        "\n",
        "次にデータセットと，パラメータの初期化を行います．\n",
        "\n",
        "今回扱うORゲートは，二つの入力の少なくともどちらかが1であれば1を出力し，入力が両方とも0であれば0を出力します．\n",
        "\n",
        "これが線形な分類器で実現可能な問題であることは，下図からもわかります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hifn1SHzEpx1"
      },
      "outputs": [],
      "source": [
        "# ORのデータセット\n",
        "x_train_or = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])  # (4, 2)\n",
        "y_train_or = np.array([[1], [1], [0], [1]])              # (4, 1)\n",
        "x_valid_or, y_valid_or = x_train_or, y_train_or\n",
        "# x_test_or, y_test_or = x_train_or, y_train_or\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.hlines([0], xmin=-1, xmax=2, color=\"black\", alpha=0.7)\n",
        "plt.vlines([0], ymin=-1, ymax=2, color=\"black\", alpha=0.7)\n",
        "\n",
        "plt.scatter(*x_train_or[y_train_or.squeeze() == 0].T, color=\"red\", label=\"0\")\n",
        "plt.scatter(*x_train_or[y_train_or.squeeze() == 1].T, color=\"blue\", label=\"1\")\n",
        "\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.xlim([-0.5, 1.5])\n",
        "plt.ylim([-0.5, 1.5])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にパラメータを初期化します．重みは一様分布からのサンプリング，バイアスは0で初期化を行います．"
      ],
      "metadata": {
        "id": "C8u9bzMEvUkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mqQyMoJCX7V"
      },
      "outputs": [],
      "source": [
        "# 重み（入力層の次元数: 2，出力層の次元数: 1）\n",
        "W_or = np.random.uniform(low=-0.08, high=0.08, size=(2, 1)).astype('float32')\n",
        "b_or = np.zeros(shape=(1,)).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7raMb3ts4KrL"
      },
      "source": [
        "### 1.3. train関数とvalid関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGygQ2Md4KrM"
      },
      "source": [
        "**目的関数**\n",
        "\n",
        "2クラス交差エントロピー誤差関数\n",
        "$$ E ({\\bf x}, {\\bf y}; {\\bf W}, {\\bf b} ) =  -\\frac{1}{N}\\sum^N_{i=1} \\left[ {\\bf y}_i \\log {\\bf \\hat{y}}_i ({\\bf x}_i; {\\bf W}, {\\bf b}) + (1 - {\\bf y}_i) \\log \\{ 1 - {\\bf \\hat{y}}_i ({\\bf x}_i; {\\bf W}, {\\bf b}) \\}\\right] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNruNkKG4KrN"
      },
      "source": [
        "**モデルの推論**\n",
        "$$\n",
        "    {\\bf \\hat{y}}_i = \\sigma({\\bf W}^T {\\bf x}_i + {\\bf b})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWW0Bsia4KrP"
      },
      "source": [
        "**モデルの学習**\n",
        "\n",
        "勾配降下法により学習します．クロスエントロピー誤差関数 $E$ のパラメータに関する勾配は以下のように簡単に計算できるので，それに基づいて更新を行っていきます．\n",
        "\n",
        "\\begin{align*}\n",
        "    \\delta_i &= {\\bf \\hat{y}}_i - {\\bf y}_i \\\\\n",
        "    \\nabla_{\\bf W} E &= \\frac{1}{N}\\sum^N_{i=1}\\delta_i {\\bf x}^{\\mathrm{T}}_i \\\\\n",
        "    \\nabla_{\\bf b} E &= \\frac{1}{N}\\sum^N_{i=1}\\delta_i  \\\\\n",
        "    {\\bf W} &\\leftarrow {\\bf W} - \\epsilon \\nabla_{\\bf W} E \\\\\n",
        "    {\\bf b} &\\leftarrow {\\bf b} - \\epsilon \\nabla_{\\bf b} E \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0NNuvbS4KrQ"
      },
      "outputs": [],
      "source": [
        "# logの中身が0になるのを防ぐ\n",
        "def np_log(x):\n",
        "    return np.log(np.clip(a=x, a_min=1e-10, a_max=1e+10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAeoMirR4KrV"
      },
      "outputs": [],
      "source": [
        "def train_or(x, y, eps=1.0):\n",
        "    \"\"\"\n",
        "    :param x: np.ndarray, 入力データ, (batch_size, 入力の次元数)\n",
        "    :param y: np.ndarray, 教師ラベル, (batch_size, 出力の次元数)\n",
        "    :param eps: float, 学習率\n",
        "    \"\"\"\n",
        "    global W_or, b_or\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # 予測\n",
        "    y_hat = sigmoid(np.matmul(x, W_or) + b_or)  # (batch_size, 出力の次元数)\n",
        "\n",
        "    # 目的関数の評価\n",
        "    cost =  # WRITE ME\n",
        "    delta = y_hat - y  # (batch_size, 出力の次元数)\n",
        "\n",
        "    # パラメータの更新\n",
        "    dW = np.matmul(x.T, delta) / batch_size  # (入力の次元数, 出力の次元数)\n",
        "    db = np.matmul(np.ones(shape=(batch_size,)), delta) / batch_size  # (出力の次元数,)\n",
        "    W_or -= eps * dW\n",
        "    b_or -= eps * db\n",
        "\n",
        "    return cost\n",
        "\n",
        "def valid_or(x, y):\n",
        "    y_hat = sigmoid(np.matmul(x, W_or) + b_or)\n",
        "    cost = (- y * np_log(y_hat) - (1 - y) * np_log(1 - y_hat)).mean()\n",
        "    return cost, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiuO_6B-4Krb"
      },
      "source": [
        "### 1.4. 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0WM2juD4Kre"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1000):\n",
        "    # x_train_or, y_train_or = shuffle(x_train_or, y_train_or)\n",
        "    cost = train_or(x_train_or, y_train_or)\n",
        "    cost, y_pred = valid_or(x_valid_or, y_valid_or)\n",
        "\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44tdPsW34Krq"
      },
      "source": [
        "## 2.【課題 2】ソフトマックス回帰の実装と学習 (MNIST)\n",
        "\n",
        "ソフトマックス回帰は，ロジスティック回帰を多クラス分類に拡張したものです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEprLDMd4Krr"
      },
      "source": [
        "### 2.1. ソフトマックス関数\n",
        "\n",
        "ソフトマックス関数は分子の全クラスにわたる和が分母になるので，モデルの出力を確率分布（多項分布）に直すものと考えることができます．\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax}({\\bf x})_k\n",
        "= \\frac{\\exp({\\bf x}_k)}{\\sum^K_{k'=1} \\exp({\\bf x}_{k'})}\n",
        "= \\frac{\\exp({\\bf x}_k - m)}{\\sum^K_{k'=1} \\exp({\\bf x}_{k'} - m)} \\hspace{10mm} \\text{for} \\, k=1,\\ldots, K\n",
        "$$\n",
        "\n",
        "ソフトマックス関数は入力に対して定数を足し引きしても結果が変わりません．上式では，定数 $m = \\max^K_{i=1} x_i$ を入力から引くことで，指数を取った時に値がオーバーフローしてしまうのを防ぐための実装を行っています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJtPrgDX4Krs"
      },
      "outputs": [],
      "source": [
        "def softmax(x, axis=1):\n",
        "    x -=  # WRITE ME# expのoverflowを防ぐ\n",
        "    x_exp =  # WRITE ME\n",
        "    return  # WRITE ME\n",
        "\n",
        "x = np.random.uniform(-5, 5, (1, 10))\n",
        "y = softmax(x.copy())\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(8, 3))\n",
        "for i, (d, title) in enumerate(zip([x, y], ['before', 'after'])):\n",
        "    axs[i].bar(np.arange(10), d.squeeze())\n",
        "    axs[i].set_title(f\"{title} softmax\")\n",
        "    axs[i].set_xticks(np.arange(10))\n",
        "    axs[i].set_xlabel(\"class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryhJ_ORtPpWV"
      },
      "source": [
        "$K=2$（2クラス）の際のソフトマックス関数を可視化してみます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV_fETERPpWV"
      },
      "outputs": [],
      "source": [
        "x = np.stack(np.meshgrid(\n",
        "    np.arange(-5, 5, 0.1),\n",
        "    np.arange(-5, 5, 0.1)\n",
        "))\n",
        "y = softmax(x.copy(), axis=0)\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.view_init(elev=30, azim=45)\n",
        "ax.plot_surface(x[0], x[1], y[0], cmap='Reds')\n",
        "ax.plot_surface(x[0], x[1], y[1], cmap='Blues')\n",
        "ax.set_xlabel(\"x_1\")\n",
        "ax.set_ylabel(\"x_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52fR-55x4Krx"
      },
      "source": [
        "### 2.2. データセットの設定と重みの定義\n",
        "\n",
        "次にデータセットを作成します．ここでは画像の多クラス分類タスクとして知られるMNISTデータセットを用います．MNISTデータセットは0-9の手書き文字の画像入力とし，画像にどの数字が書かれているかを出力する10クラス分類のタスクです．入力データは以下のようなグレースケールの画像が入力されます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFxomfOd_gzI"
      },
      "outputs": [],
      "source": [
        "(x_mnist_1, y_mnist_1), (x_mnist_2, y_mnist_2) = mnist.load_data()\n",
        "\n",
        "x_mnist = np.r_[x_mnist_1, x_mnist_2]\n",
        "y_mnist = np.r_[y_mnist_1, y_mnist_2]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i in range(20):\n",
        "    x = x_mnist[i]\n",
        "    ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(x, 'gray')\n",
        "fig.tight_layout(rect=[0,0,1,0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB3z2eKuOwsz"
      },
      "outputs": [],
      "source": [
        "x_mnist = x_mnist.astype('float32') / 255.  # 画素値のスケーリング: 0 - 255 (整数) → 0.0 - 1.0 (浮動小数点数)\n",
        "y_mnist = np.eye(N=10)[y_mnist.astype('int32').flatten()]  # 整数ラベル (0–9) を one-hot ベクトル に変換\n",
        "\n",
        "x_mnist=x_mnist.reshape(x_mnist.shape[0],-1)  # 画素の平坦化: 28 * 28 (2次元画像) → 784 (1次元画像)\n",
        "\n",
        "x_train_mnist, x_test_mnist, y_train_mnist, y_test_mnist = train_test_split(x_mnist, y_mnist, test_size=10000)\n",
        "x_train_mnist, x_valid_mnist, y_train_mnist, y_valid_mnist = train_test_split(x_train_mnist, y_train_mnist, test_size=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPX8RF6H4Kr2"
      },
      "outputs": [],
      "source": [
        "W_mnist = np.random.uniform(low=-0.08, high=0.08, size=(784, 10)).astype('float32')  # 重み: (784, 10)\n",
        "b_mnist = np.zeros(shape=(10,)).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80EOFI-n4Kr6"
      },
      "source": [
        "### 2.3. train関数とvalid関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVyNaUd-4Kr8"
      },
      "source": [
        "**目的関数**\n",
        "\n",
        "多クラス交差エントロピー誤差関数\n",
        "$$ E ({\\bf x}, {\\bf y}; {\\bf W}, {\\bf b} ) =  -\\frac{1}{N}\\sum^N_{i=1} \\sum^K_{k=1} {\\bf y}_{i, k} \\log {\\bf \\hat{y}}_{i, k} ({\\bf x}_i; {\\bf W}, {\\bf b}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZIHaLH4Kr9"
      },
      "source": [
        "**モデルの推論**\n",
        "$$\n",
        "    {\\bf \\hat{y}}_i = \\mathrm{softmax}({\\bf W}^T{\\bf x}_i + {\\bf b})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65ew-Qs4Kr-"
      },
      "source": [
        "**モデルの学習**\n",
        "\\begin{align*}\n",
        "    \\delta_i &= {\\bf \\hat{y}}_i - {\\bf y}_i \\\\\n",
        "    \\nabla_{\\bf W} E &= \\frac{1}{N}\\sum^N_{i=1}x_i {\\bf \\delta}^{\\mathrm{T}}_i \\\\\n",
        "    \\nabla_{\\bf b} E &= \\frac{1}{N}\\sum^N_{i=1}\\delta_i  \\\\\n",
        "    {\\bf W} &\\leftarrow {\\bf W} - \\epsilon \\nabla_{\\bf W} E \\\\\n",
        "    {\\bf b} &\\leftarrow {\\bf b} - \\epsilon \\nabla_{\\bf b} E \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ems70ikO4KsA"
      },
      "outputs": [],
      "source": [
        "def train_mnist(x, y, eps=1.0):\n",
        "    \"\"\"\n",
        "    :param x: np.ndarray, 入力データ, (batch_size, 入力の次元数)\n",
        "    :param y: np.ndarray, 教師ラベル, (batch_size, 出力の次元数)\n",
        "    :param eps: float, 学習率\n",
        "    \"\"\"\n",
        "    global W_mnist, b_mnist\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # 予測\n",
        "    y_hat = softmax(np.matmul(x, W_mnist) + b_mnist)  # (batch_size, 出力の次元数)\n",
        "\n",
        "    # 目的関数の評価\n",
        "    cost = (- y * np_log(y_hat)).sum(axis=1).mean()\n",
        "    delta = y_hat - y  # (batch_size, 出力の次元数)\n",
        "\n",
        "    # パラメータの更新\n",
        "    dW = np.matmul(x.T, delta) / batch_size  # (入力の次元数, 出力の次元数)\n",
        "    db = np.matmul(np.ones(shape=(batch_size,)), delta) / batch_size  # (出力の次元数,)\n",
        "    W_mnist -= eps * dW\n",
        "    b_mnist -= eps * db\n",
        "\n",
        "    return cost\n",
        "\n",
        "def valid_mnist(x, y):\n",
        "    y_hat = softmax(np.matmul(x, W_mnist) + b_mnist)\n",
        "    cost = (- y * np_log(y_hat)).sum(axis=1).mean()\n",
        "\n",
        "    return cost, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGInXhG4KsJ"
      },
      "source": [
        "### 2.4. 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NbmLU4A4KsN"
      },
      "outputs": [],
      "source": [
        "for epoch in range(100):\n",
        "    cost = train_mnist(x_train_mnist, y_train_mnist)\n",
        "\n",
        "    cost, y_pred = valid_mnist(x_valid_mnist, y_valid_mnist)\n",
        "    if epoch % 10 == 9 or epoch == 0:\n",
        "        print('EPOCH: {}, Valid Cost: {:.3f}, Valid Accuracy: {:.3f}'.format(\n",
        "            epoch + 1,\n",
        "            cost,\n",
        "            accuracy_score(y_valid_mnist.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "        ))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8ec987ac23ada7ab2406f4681db1dc9cb7c577fa45cbbdbd656476bb3e65da57"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}